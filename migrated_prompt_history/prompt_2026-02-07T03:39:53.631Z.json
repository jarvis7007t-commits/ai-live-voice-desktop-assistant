[
  {
    "id": 0,
    "author": "user",
    "payload": {
      "type": "text",
      "text": "Storage is now 23GB free. I fixed the space issue. Please convert my current project into a Floating Capsule UI:\n\nIn electron.js: Set the window to 450x80 size. Use frame: false, transparent: true, alwaysOnTop: true.\n\nIn CSS: Make the background fully transparent. Style the main wrapper as a dark rounded pill (radius 50px) with -webkit-app-region: drag;.\n\nFix any SyntaxErrors in the configuration files if they exist.",
      "files": [],
      "fileUploads": [],
      "cssBlocks": [],
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770395266,
      "nanos": 850000000
    }
  },
  {
    "id": 1,
    "author": "model",
    "payload": {
      "type": "thinking",
      "text": "**Designing the Capsule UI**\n\nI'm now fully immersed in the UI design. I'm focusing on creating a floating capsule using Electron.js. The plan is a window with dimensions of 450x80, frameless and transparent, and set to always be on top. CSS/HTML will handle the rest of the visual details.\n\n\n**Refining the Visuals**\n\nI'm now zeroing in on the aesthetic design of the capsule. My focus is on achieving a dark, rounded pill shape with glassmorphism or neon effects. I'm adjusting the CSS in `index.html` to guarantee strict transparency for the `html` and `body` elements. The `.lumina-capsule` class in `App.tsx` will house the primary styling. Finally, I'm updating `electron.js` to ensure the dimensions of 450x80 are properly set, as well as fixing the messy import map. I've updated the constants in `createMainWindow`.\n\n\n**Adjusting the Layout**\n\nI'm now revising the layout to fit the 450x80 container. I'm focusing on ensuring the JSX structure in `App.tsx` adapts to this new size. I'm thinking about the visual impact and am considering the `esm.sh` imports from `index.html`, and their potential consistency needs. The `package.json` setup is as expected. I'm also now setting `winWidth` to 450 and `winHeight` to 80 in `electron.js`, and adjusting the `.lumina-capsule` height to 60px in `index.html` to allow for glow effects. I'm prioritizing a premium look for the start button and controls within the 80px height.\n\n\n",
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770395266,
      "nanos": 850000000
    },
    "doneTimestamp": {
      "seconds": 1770395280,
      "nanos": 460000000
    }
  },
  {
    "id": 2,
    "author": "model",
    "payload": {
      "type": "text",
      "text": "This update transforms the application into a highly refined, high-performance Floating Capsule UI. \n\n### Specification:\n1.  **Electron Optimization**: The main window is now precisely 450x80, frameless, and fully transparent with `alwaysOnTop` enabled to serve as a persistent desktop overlay.\n2.  **Aesthetic \"Capsule\" Styling**: \n    *   The main container is a dark, glass-morphic pill with a `50px` border-radius.\n    *   Integrated `-webkit-app-region: drag` allows for smooth movement anywhere on the screen.\n    *   The visualizer is centered and expanded to occupy the mid-section of the 450px width.\n3.  **Visual Polish**: \n    *   Added a refined \"Vortex\" glow that pulses when the connection is active.\n    *   The capsule now includes a subtle \"breathing\" animation when the AI is processing or speaking.\n    *   Controls are simplified to minimize clutter within the compact 80px height.\n\n",
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770395280,
      "nanos": 461000000
    },
    "doneTimestamp": {
      "seconds": 1770395280,
      "nanos": 673000000
    }
  },
  {
    "id": 3,
    "author": "model",
    "payload": {
      "type": "generationTable",
      "entries": [
        {
          "path": "electron.js",
          "generationType": 1,
          "isDone": true,
          "description": "Update window dimensions to 450x80 and ensure proper transparency and drag behavior.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nconst { app, BrowserWindow, ipcMain, screen, Tray, Menu } = require('electron');\nconst path = require('path');\n\nlet mainWindow;\nlet videoWindow;\nlet tray = null;\nlet isQuiting = false;\n\nconst VERCEL_URL = 'http://localhost:3000';\n\nfunction createTray() {\n  const iconPath = path.join(__dirname, 'icon.ico');\n  try {\n    tray = new Tray(iconPath);\n  } catch (e) {\n    console.warn(\"Tray icon not found, skipping tray creation.\");\n    return;\n  }\n  \n  const contextMenu = Menu.buildFromTemplate([\n    { \n      label: 'Show Assistant', \n      click: () => {\n        if (mainWindow) {\n          mainWindow.show();\n          mainWindow.focus();\n        }\n      } \n    },\n    { type: 'separator' },\n    { \n      label: 'Quit Entirely', \n      click: () => {\n        isQuiting = true;\n        app.quit();\n      } \n    }\n  ]);\n\n  tray.setToolTip('Gemini Live Assistant');\n  tray.setContextMenu(contextMenu);\n\n  tray.on('double-click', () => {\n    if (mainWindow) mainWindow.show();\n  });\n}\n\nfunction createMainWindow() {\n  const { width: screenWidth, height: screenHeight } = screen.getPrimaryDisplay().workAreaSize;\n\n  // Optimized for Floating Capsule UI (450x80)\n  const winWidth = 450;\n  const winHeight = 80;\n  const margin = 40;\n\n  mainWindow = new BrowserWindow({\n    width: winWidth,\n    height: winHeight,\n    x: screenWidth - winWidth - margin,\n    y: screenHeight - winHeight - margin,\n    frame: false,\n    transparent: true,\n    alwaysOnTop: true,\n    resizable: false,\n    movable: true,\n    hasShadow: false,\n    backgroundColor: '#00000000', // Fully transparent\n    webPreferences: {\n      nodeIntegration: true,\n      contextIsolation: false,\n      backgroundThrottling: false,\n      webSecurity: false\n    },\n  });\n\n  mainWindow.setAlwaysOnTop(true, 'screen-saver');\n\n  const startUrl = process.env.ELECTRON_START_URL || VERCEL_URL;\n  mainWindow.loadURL(startUrl);\n\n  mainWindow.on('close', (event) => {\n    if (!isQuiting) {\n      event.preventDefault();\n      mainWindow.hide();\n    }\n    return false;\n  });\n\n  mainWindow.on('closed', () => {\n    mainWindow = null;\n    if (videoWindow) videoWindow.close();\n  });\n}\n\nfunction createVideoWindow() {\n  if (videoWindow) {\n    videoWindow.focus();\n    return;\n  }\n\n  videoWindow = new BrowserWindow({\n    width: 640,\n    height: 480,\n    title: \"Live Vision Feed\",\n    autoHideMenuBar: true,\n    backgroundColor: '#000000',\n    webPreferences: {\n      nodeIntegration: true,\n      contextIsolation: false,\n    },\n  });\n\n  const baseAppUrl = process.env.ELECTRON_START_URL || VERCEL_URL;\n  const videoUrl = baseAppUrl.includes('?') ? `${baseAppUrl}&view=camera` : `${baseAppUrl}?view=camera`;\n  \n  videoWindow.loadURL(videoUrl);\n\n  videoWindow.on('closed', () => {\n    videoWindow = null;\n  });\n}\n\nipcMain.on('resize-window', (event, expand) => {\n  // Maintaining consistent size for the capsule design\n  if (mainWindow) {\n    mainWindow.setSize(450, 80, true);\n  }\n});\n\nipcMain.on('open-video-window', () => {\n  createVideoWindow();\n});\n\napp.whenReady().then(() => {\n  createMainWindow();\n  createTray();\n\n  app.on('activate', () => {\n    if (BrowserWindow.getAllWindows().length === 0) createMainWindow();\n    else mainWindow.show();\n  });\n});\n\napp.on('window-all-closed', () => {\n  if (process.platform !== 'darwin') {\n    // App stays in tray\n  }\n});\n\napp.on('before-quit', () => {\n  isQuiting = true;\n});\n"
            }
          ]
        },
        {
          "path": "index.html",
          "generationType": 1,
          "isDone": true,
          "description": "Refine global CSS for the Floating Capsule UI with 50px radius and proper transparency.",
          "diffs": [
            {
              "target": "",
              "replacement": "\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" />\n    <title>AI Live Voice Desktop Assistant</title>\n    <meta name=\"theme-color\" content=\"#000000\" />\n    <link rel=\"manifest\" href=\"/manifest.json\">\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap\" rel=\"stylesheet\">\n    <style>\n      * { box-sizing: border-box; -webkit-font-smoothing: antialiased; }\n      html, body {\n        height: 100%; width: 100%; margin: 0; padding: 0;\n        background: transparent !important; \n        overflow: hidden; \n      }\n      body { \n        font-family: 'Inter', sans-serif; \n        display: flex; \n        align-items: center; \n        justify-content: center;\n        background: transparent !important;\n      }\n      #root { \n        width: 100%; \n        height: 100%; \n        display: flex; \n        align-items: center; \n        justify-content: center; \n        background: transparent !important;\n      }\n\n      /* Floating Capsule UI: Refined Pill Shape */\n      .lumina-capsule {\n        position: relative;\n        width: 420px; /* Leave some margin for the 450px window */\n        height: 64px;\n        border-radius: 50px; /* Required 50px radius */\n        display: flex;\n        align-items: center;\n        justify-content: space-between;\n        padding: 0 16px;\n        background: linear-gradient(180deg, rgba(16, 20, 29, 0.95) 0%, rgba(5, 7, 10, 0.98) 100%);\n        border: 1px solid rgba(34, 211, 238, 0.3);\n        box-shadow: \n          0 8px 32px rgba(0, 0, 0, 0.8), \n          0 0 15px rgba(34, 211, 238, 0.15),\n          inset 0 0 12px rgba(255, 255, 255, 0.05);\n        z-index: 10;\n        transition: all 0.4s cubic-bezier(0.2, 1, 0.3, 1);\n        -webkit-app-region: drag; /* Dragging enabled */\n      }\n\n      @keyframes breathe {\n        0%, 100% { transform: scale(1); border-color: rgba(34, 211, 238, 0.3); }\n        50% { transform: scale(1.01); border-color: rgba(34, 211, 238, 0.6); }\n      }\n\n      .vibrating {\n        animation: breathe 2s infinite ease-in-out;\n        box-shadow: \n          0 8px 32px rgba(0, 0, 0, 0.9), \n          0 0 25px rgba(34, 211, 238, 0.4),\n          inset 0 0 12px rgba(255, 255, 255, 0.1);\n      }\n\n      .section-vortex {\n        position: relative;\n        width: 44px;\n        height: 44px;\n        display: flex;\n        justify-content: center;\n        align-items: center;\n        cursor: pointer;\n        border-radius: 50%;\n        background: rgba(255, 255, 255, 0.05);\n        border: 1px solid rgba(255, 255, 255, 0.1);\n        flex-shrink: 0;\n        transition: transform 0.2s ease;\n        -webkit-app-region: no-drag;\n      }\n      .section-vortex:hover { transform: scale(1.1); background: rgba(34, 211, 238, 0.1); }\n\n      .vortex-glow {\n        position: absolute;\n        width: 150%;\n        height: 150%;\n        background: radial-gradient(circle, rgba(34, 211, 238, 0.5) 0%, transparent 70%);\n        border-radius: 50%;\n        filter: blur(10px);\n        animation: pulse-glow 2.5s infinite ease-in-out;\n        opacity: 0;\n        transition: opacity 0.5s ease;\n      }\n\n      .connected .vortex-glow { opacity: 1; }\n\n      @keyframes pulse-glow {\n        0%, 100% { opacity: 0.3; transform: scale(0.9); }\n        50% { opacity: 0.7; transform: scale(1.2); }\n      }\n\n      .section-controls {\n        display: flex;\n        gap: 8px;\n        align-items: center;\n        flex-shrink: 0;\n        -webkit-app-region: no-drag;\n      }\n\n      .control-icon {\n        width: 32px;\n        height: 32px;\n        transition: all 0.2s ease;\n        cursor: pointer;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        position: relative;\n        border-radius: 50%;\n        background: rgba(255, 255, 255, 0.05);\n        color: #94a3b8;\n      }\n\n      .control-icon:hover { background: rgba(255, 255, 255, 0.15); color: #fff; transform: translateY(-2px); }\n      .icon-active-cyan { color: #22d3ee !important; filter: drop-shadow(0 0 8px rgba(34, 211, 238, 0.8)); }\n      .icon-active-red { color: #ef4444 !important; filter: drop-shadow(0 0 10px rgba(239, 68, 68, 0.8)); }\n      \n      .slashed::after {\n        content: \"\";\n        position: absolute;\n        width: 20px;\n        height: 2px;\n        background: #ef4444;\n        transform: rotate(-45deg);\n        border-radius: 2px;\n      }\n    </style>\n  <script type=\"importmap\">\n{\n  \"imports\": {\n    \"react\": \"https://esm.sh/react@^19.2.4\",\n    \"react-dom\": \"https://esm.sh/react-dom@^19.2.4\",\n    \"react-dom/\": \"https://esm.sh/react-dom@^19.2.4/\",\n    \"react/\": \"https://esm.sh/react@^19.2.4/\",\n    \"@google/genai\": \"https://esm.sh/@google/genai@^1.40.0\"\n  }\n}\n</script>\n</head>\n  <body>\n    <div id=\"root\"></div>\n    <script type=\"module\" src=\"/index.tsx\"></script>\n  </body>\n</html>\n"
            }
          ]
        },
        {
          "path": "App.tsx",
          "generationType": 1,
          "isDone": true,
          "description": "Refine App layout for the 450x80 Floating Capsule, ensuring the visualizer is prominent.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport React, { useState, useRef, useCallback, useEffect, useMemo } from 'react';\nimport { GoogleGenAI, Modality, LiveServerMessage } from '@google/genai';\nimport { SessionStatus, LiveConfig } from './types';\nimport { createBlob, decode, decodeAudioData } from './utils/audio-utils';\nimport Visualizer from './components/Visualizer';\n\nconst isElectron = typeof window !== 'undefined' && (window as any).process && (window as any).process.type;\nconst ipcRenderer = isElectron ? (window as any).require('electron').ipcRenderer : null;\n\nconst MODEL_NAME = 'gemini-2.5-flash-native-audio-preview-12-2025';\nconst FRAME_RATE = 2; \n\nconst App: React.FC = () => {\n  const queryParams = new URLSearchParams(window.location.search);\n  const isCameraView = queryParams.get('view') === 'camera';\n\n  // State Sync Channel\n  const syncChannel = useMemo(() => new BroadcastChannel('gemini-sync'), []);\n\n  // --- VIDEO CALL WINDOW UI ---\n  if (isCameraView) {\n    const camRef = useRef<HTMLVideoElement>(null);\n    const [zoom, setZoom] = useState(1);\n    const [isMirrored, setIsMirrored] = useState(true);\n    const [isMuted, setIsMuted] = useState(false);\n    const [isConnected, setIsConnected] = useState(false);\n    const [seconds, setSeconds] = useState(0);\n    const [isModelTalking, setIsModelTalking] = useState(false);\n    const [error, setError] = useState<string | null>(null);\n\n    useEffect(() => {\n      syncChannel.onmessage = (e) => {\n        if (e.data.type === 'STATE_UPDATE') {\n          setIsMuted(e.data.isMuted);\n          setIsConnected(e.data.status === 'CONNECTED');\n          setIsModelTalking(e.data.isModelTalking);\n        }\n      };\n      return () => syncChannel.close();\n    }, [syncChannel]);\n\n    useEffect(() => {\n      let interval: number;\n      if (isConnected) {\n        interval = window.setInterval(() => setSeconds(s => s + 1), 1000);\n      }\n      return () => clearInterval(interval);\n    }, [isConnected]);\n\n    useEffect(() => {\n      async function setupCam() {\n        try {\n          const devices = await navigator.mediaDevices.enumerateDevices();\n          const hasVideo = devices.some(device => device.kind === 'videoinput');\n          if (!hasVideo) { setError(\"No camera detected on this system.\"); return; }\n          const stream = await navigator.mediaDevices.getUserMedia({ \n            video: { width: { ideal: 1280 }, height: { ideal: 720 } } \n          }).catch(() => navigator.mediaDevices.getUserMedia({ video: true }));\n          if (camRef.current) camRef.current.srcObject = stream;\n        } catch (e: any) {\n          setError(e.message || \"Failed to access camera.\");\n        }\n      }\n      setupCam();\n      return () => {\n        const stream = camRef.current?.srcObject as MediaStream;\n        stream?.getTracks().forEach(t => t.stop());\n      };\n    }, []);\n\n    const handleEndCall = () => {\n      syncChannel.postMessage({ type: 'COMMAND', action: 'STOP' });\n      window.close();\n    };\n\n    return (\n      <div className=\"relative w-full h-full bg-[#05070a] flex items-center justify-center overflow-hidden font-sans select-none text-white\">\n        <div className={`absolute inset-0 transition-all duration-700 ${isConnected && !error ? 'scale-100 opacity-100' : 'scale-110 opacity-40 blur-md'}`}>\n           <video \n            ref={camRef} \n            autoPlay \n            playsInline \n            muted \n            className=\"w-full h-full object-cover\"\n            style={{ transform: `scale(${zoom}) ${isMirrored ? 'scaleX(-1)' : 'scaleX(1)'}` }}\n          />\n        </div>\n        <div className=\"absolute top-0 left-0 right-0 p-6 flex justify-between items-start bg-gradient-to-b from-black/80 to-transparent\">\n          <div className=\"flex flex-col gap-1\">\n            <h1 className=\"text-lg font-semibold tracking-tight flex items-center gap-2\">\n               <div className={`w-2 h-2 rounded-full ${isConnected ? 'bg-cyan-500 animate-pulse' : 'bg-zinc-500'}`}></div>\n               Live Vision Link\n            </h1>\n          </div>\n          <button onClick={handleEndCall} className=\"bg-red-600 px-4 py-2 rounded-lg text-sm font-bold\">DISCONNECT</button>\n        </div>\n      </div>\n    );\n  }\n\n  // --- MAIN FLOATING CAPSULE UI ---\n  const [status, setStatus] = useState<SessionStatus>(SessionStatus.IDLE);\n  const [config, setConfig] = useState<LiveConfig>({\n    model: MODEL_NAME,\n    voiceName: 'Zephyr',\n    isCameraEnabled: false,\n    isMuted: false,\n    isMouseMode: true\n  });\n  \n  const [isUserTalking, setIsUserTalking] = useState(false);\n  const [isModelTalking, setIsModelTalking] = useState(false);\n  \n  const isMutedRef = useRef(config.isMuted);\n  const sessionRef = useRef<any>(null);\n  const videoRef = useRef<HTMLVideoElement>(null);\n  const canvasRef = useRef<HTMLCanvasElement>(null);\n  const frameIntervalRef = useRef<number | null>(null);\n\n  useEffect(() => { isMutedRef.current = config.isMuted; }, [config.isMuted]);\n\n  const audioContextRef = useRef<{ input: AudioContext; output: AudioContext } | null>(null);\n  const audioNodesRef = useRef<{ source?: MediaStreamAudioSourceNode; processor?: ScriptProcessorNode } | null>(null);\n  const audioSourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());\n  const nextStartTimeRef = useRef(0);\n\n  useEffect(() => {\n    syncChannel.postMessage({ type: 'STATE_UPDATE', isMuted: config.isMuted, status, isModelTalking });\n  }, [config.isMuted, status, isModelTalking, syncChannel]);\n\n  useEffect(() => {\n    syncChannel.onmessage = (e) => {\n      if (e.data.type === 'COMMAND') {\n        if (e.data.action === 'STOP') stopSession();\n        if (e.data.action === 'TOGGLE_MUTE') setConfig(c => ({...c, isMuted: e.data.value}));\n      }\n    };\n  }, [syncChannel]);\n\n  const stopSession = useCallback(() => {\n    if (frameIntervalRef.current) window.clearInterval(frameIntervalRef.current);\n    if (sessionRef.current) sessionRef.current.close?.();\n    sessionRef.current = null;\n    if (audioNodesRef.current?.processor) audioNodesRef.current.processor.disconnect();\n    if (audioNodesRef.current?.source) audioNodesRef.current.source.disconnect();\n    audioSourcesRef.current.forEach(s => { try { s.stop(); } catch(e) {} });\n    audioSourcesRef.current.clear();\n    const stream = videoRef.current?.srcObject as MediaStream;\n    stream?.getTracks().forEach(track => track.stop());\n    setStatus(SessionStatus.IDLE);\n    setIsUserTalking(false);\n    setIsModelTalking(false);\n  }, []);\n\n  const startVisionLoop = useCallback((session: any) => {\n    if (frameIntervalRef.current) window.clearInterval(frameIntervalRef.current);\n    frameIntervalRef.current = window.setInterval(async () => {\n      if (!videoRef.current || !canvasRef.current) return;\n      const video = videoRef.current;\n      const canvas = canvasRef.current;\n      const ctx = canvas.getContext('2d');\n      if (!ctx || video.paused || video.ended) return;\n      canvas.width = 640; canvas.height = 480;\n      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n      const base64Data = canvas.toDataURL('image/jpeg', 0.5).split(',')[1];\n      if (session) session.sendRealtimeInput({ media: { data: base64Data, mimeType: 'image/jpeg' } });\n    }, 1000 / FRAME_RATE);\n  }, []);\n\n  const startSession = async () => {\n    try {\n      setStatus(SessionStatus.CONNECTING);\n      const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });\n      if (!audioContextRef.current) {\n        audioContextRef.current = {\n          input: new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 }),\n          output: new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 }),\n        };\n      }\n      const { input: inputCtx, output: outputCtx } = audioContextRef.current;\n      await inputCtx.resume(); await outputCtx.resume();\n      const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });\n\n      const sessionPromise = ai.live.connect({\n        model: MODEL_NAME,\n        config: {\n          responseModalities: [Modality.AUDIO],\n          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: config.voiceName } } },\n          systemInstruction: \"You are a desktop AI assistant. Be concise, witty, and extremely helpful.\"\n        },\n        callbacks: {\n          onopen: () => {\n            setStatus(SessionStatus.CONNECTED);\n            const source = inputCtx.createMediaStreamSource(audioStream);\n            const scriptProcessor = inputCtx.createScriptProcessor(4096, 1, 1);\n            scriptProcessor.onaudioprocess = (e) => {\n              if (isMutedRef.current) { setIsUserTalking(false); return; }\n              const inputData = e.inputBuffer.getChannelData(0);\n              const sum = inputData.reduce((a, b) => a + Math.abs(b), 0);\n              setIsUserTalking(sum / inputData.length > 0.015);\n              sessionPromise.then(s => s.sendRealtimeInput({ media: createBlob(inputData) })).catch(() => {});\n            };\n            source.connect(scriptProcessor);\n            scriptProcessor.connect(inputCtx.destination);\n            audioNodesRef.current = { source, processor: scriptProcessor };\n          },\n          onmessage: async (message: LiveServerMessage) => {\n            const audioData = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;\n            if (audioData) {\n              setIsModelTalking(true);\n              nextStartTimeRef.current = Math.max(nextStartTimeRef.current, outputCtx.currentTime);\n              const audioBuffer = await decodeAudioData(decode(audioData), outputCtx, 24000, 1);\n              const source = outputCtx.createBufferSource();\n              source.buffer = audioBuffer;\n              source.connect(outputCtx.destination);\n              source.addEventListener('ended', () => {\n                audioSourcesRef.current.delete(source);\n                if (audioSourcesRef.current.size === 0) setIsModelTalking(false);\n              });\n              source.start(nextStartTimeRef.current);\n              nextStartTimeRef.current += audioBuffer.duration;\n              audioSourcesRef.current.add(source);\n            }\n          },\n          onerror: () => stopSession(),\n          onclose: () => stopSession(),\n        },\n      });\n      sessionRef.current = await sessionPromise;\n    } catch (err) { setStatus(SessionStatus.IDLE); }\n  };\n\n  const isConnected = status === SessionStatus.CONNECTED;\n\n  return (\n    <div className={`lumina-capsule ${isConnected ? 'connected' : ''} ${isModelTalking ? 'vibrating' : ''}`}>\n      <video ref={videoRef} autoPlay playsInline muted className=\"hidden\" />\n      <canvas ref={canvasRef} className=\"hidden\" />\n\n      {/* Action Button (Vortex) */}\n      <div className=\"section-vortex\" onClick={isConnected ? stopSession : startSession}>\n        <div className=\"vortex-glow\"></div>\n        <svg className=\"relative z-10\" width=\"22\" height=\"22\" viewBox=\"0 0 24 24\" fill=\"none\" stroke={isConnected ? \"#22d3ee\" : \"#fff\"} strokeWidth=\"2\" strokeLinecap=\"round\" strokeLinejoin=\"round\">\n          <circle cx=\"12\" cy=\"12\" r=\"10\"></circle>\n          <path d=\"M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z\"></path>\n        </svg>\n      </div>\n\n      {/* Center Visualization Area */}\n      <div className=\"flex-grow flex items-center justify-center px-4 overflow-hidden h-full\">\n        <Visualizer \n          isActive={isConnected} \n          isUserTalking={isUserTalking} \n          isModelTalking={isModelTalking} \n          isMuted={config.isMuted} \n        />\n      </div>\n\n      {/* System Controls */}\n      <div className=\"section-controls\">\n        <div \n          className={`control-icon ${config.isMuted ? 'slashed' : 'icon-active-cyan'}`}\n          onClick={() => setConfig(p => ({...p, isMuted: !p.isMuted}))}\n        >\n          <svg width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" strokeWidth=\"2\"><path d=\"M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z\"/><path d=\"M19 10v2a7 7 0 0 1-14 0v-2\"/></svg>\n        </div>\n        <div \n          className=\"control-icon hover:text-cyan-400\"\n          onClick={() => { if(ipcRenderer) ipcRenderer.send('open-video-window'); }}\n        >\n          <svg width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" strokeWidth=\"2\"><path d=\"M23 7l-7 5 7 5V7z\"/><rect x=\"1\" y=\"5\" width=\"15\" height=\"14\" rx=\"2\" ry=\"2\"></rect></svg>\n        </div>\n        <div className=\"w-[1px] h-6 bg-white/10 mx-1\"></div>\n        <div \n          className=\"control-icon icon-active-red\"\n          onClick={isConnected ? stopSession : () => window.close()}\n        >\n          <svg width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"currentColor\"><path d=\"M22 16.92v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.5 19.5 0 0 1-6-6 19.79 19.79 0 0 1-3.07-8.67A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91a16 16 0 0 0 6 6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7A2 2 0 0 1 22 16.92z\"></path></svg>\n        </div>\n      </div>\n    </div>\n  );\n};\n\nexport default App;\n"
            }
          ]
        }
      ]
    },
    "createdTimestamp": {
      "seconds": 1770395282,
      "nanos": 303000000
    },
    "doneTimestamp": {
      "seconds": 1770395288,
      "nanos": 91000000
    }
  }
]